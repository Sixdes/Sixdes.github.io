{"meta":{"title":"Yao's station","subtitle":"Found in confusion.","description":"Found in confusion.","author":"Sixdes","url":"http://yaoyirong.cn"},"pages":[{"title":"标签","date":"2016-11-27T13:35:38.000Z","updated":"2017-04-13T12:38:54.000Z","comments":true,"path":"archive/index.html","permalink":"http://yaoyirong.cn/archive/index.html","excerpt":"","text":""},{"title":"","date":"2018-03-19T14:36:09.000Z","updated":"2018-03-19T14:36:09.000Z","comments":false,"path":"tags/index.html","permalink":"http://yaoyirong.cn/tags/index.html","excerpt":"","text":""},{"title":"about","date":"2018-03-19T14:45:45.000Z","updated":"2018-03-19T14:45:45.000Z","comments":true,"path":"about/index-1.html","permalink":"http://yaoyirong.cn/about/index-1.html","excerpt":"","text":""},{"title":"about","date":"2018-03-19T14:45:22.000Z","updated":"2018-03-19T14:45:22.000Z","comments":true,"path":"about/index.html","permalink":"http://yaoyirong.cn/about/index.html","excerpt":"","text":""}],"posts":[{"title":"caffe Feature extraction","slug":"Feature_extraction_caffe","date":"2018-01-03T05:42:00.000Z","updated":"2018-01-06T08:44:46.000Z","comments":true,"path":"2018/01/03/Feature_extraction_caffe/","link":"","permalink":"http://yaoyirong.cn/2018/01/03/Feature_extraction_caffe/","excerpt":"caffe 官方示例","text":"caffe 官方示例 Feature_extraction(caffe示例1)尝试使用caffe, TensorFlow等框架来实现图像特征提取 使用Caffe官方提供的深度模型——CaffeNet来演示图像识别与分类。 使用预先训练的模型进行即时识别，并通过网络界面逐层浏览特征和参数。 下载CaffeNet模型1234567891011121314151617181920212223import numpy as npimport matplotlib.pyplot as plt%matplotlib inline#设置默认显示参数plt.rcParams['figure.figsize'] = (10, 10) # 图像显示大小plt.rcParams['image.interpolation'] = 'nearest' # 最近邻差值: 像素为正方形plt.rcParams['image.cmap'] = 'gray' # 使用灰度输出而不是彩色输出# 这里我们将把caffe 模块添加到Python路径下.import syscaffe_root = '/usr/local/Cellar/caffe/' # caffe根目录，此处为相对路径，如果失灵，可换成绝对路径# sys.path是一个列表，insert()函数插入一行，也可以使用sys.path.append('模块地址')# sys.path.insert(0, caffe_root + 'python') # 加载caffe的python模块import caffeimport osif os.path.isfile(caffe_root + 'models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel'): print('caffenet fount')else: print('Downloading pre-trained CaffeNet model...') 1caffenet fount 加载 net 以及输入图像预处理 Set Caffe to CPU mode and load the net from disk. 12345678caffe.set_mode_cpu()model_def = caffe_root + 'models/bvlc_reference_caffenet/deploy.prototxt'model_weights = caffe_root + 'models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel'net = caffe.Net(model_def, # 定义模型结构 model_weights, # 包含了模型的训练权值 caffe.TEST) # 使用测试模式(不执行dropout) 设置输入预处理(我们将使用 caffe.io.Transformer 来进行预处理.由于该步骤与caffe的其它模块相互独立，所以任何预处理代码应该都是可行的) CaffeNet模型默认的输入图像格式是BGR格式, 其像素值位于[0,255]之间, 同时每个像素值都减去了ImageNet图像的平均值. 此外, 通道的维数等于第一维(outermost最外面的)大小 由于matplotlib加载的图像的值位于[0,1]之间，并且格式是RGB格式，通道的维数等于innermost(最里面的)维数，所以需要做相应变换: 图像变换1234567891011121314151617181920212223242526# 加载ImageNet训练集(同caffe一起发布)的图像均值，预处理需要减去均值# 加载均值文件mu = np.load(caffe_root + 'python/caffe/imagenet/ilsvrc_2012_mean.npy') # mu.shape (3, 256, 256)# 对所有像素值取平均以此获取BGR的均值像素值mu = mu.mean(1).mean(1)print('mean-subtracted values: ', list(zip('BGR', mu)))# 对输入数据进行变换# caffe.io.transformer是一个类，实体化的时候构造函数__init__(self, inputs)给一个初值# 其中net.blobs本身是一个字典，每一个key对应每一层的名字# net.blobs['data'].data.shape计算结果为(10, 3, 227, 227)# print(net.blobs['data'].data.shape)transformer = caffe.io.Transformer(&#123;'data': net.blobs['data'].data.shape&#125;)# 以下都是caffe.io.transformer类的函数方法# caffe.io.transformer的类定义放在io.py文件中，也可用help函数查看说明# 将图像通道数设置为outermost的维数transformer.set_transpose('data', (2, 0, 1)) # 每个通道都减去BGR的均值像素值transformer.set_mean('data', mu)# 像素值从[0,1]变换为[0,255]transformer.set_raw_scale('data', 255)# 交换通道，RGB-&gt;BGRtransformer.set_channel_swap('data', (2, 1, 0)) 1mean-subtracted values: [(&apos;B&apos;, 104.0069879317889), (&apos;G&apos;, 116.66876761696767), (&apos;R&apos;, 122.6789143406786)] CPU 分类设置输入图像大小(可跳)现在我们开始进行分类. 尽管我们只对一张图像进行分类, 不过我们将batch的大小设置为50以此来演示batching(一批). 1234# 设置输入图像大小(可跳, 以后也可以改)net.blobs['data'].reshape(50, # batch size 3, # 3-channel (BGR) images 227, 227) # # image size is 227x227 加载图像(caffe自带的)并进行预处理12345678# image RGB 0-1 innermostimage = caffe.io.load_image(caffe_root + 'examples/images/cat.jpg')print(image.shape)# transformed_image BGR 0-255 outermosttransformed_image = transformer.preprocess('data', image)print(transformed_image.shape)plt.imshow(image) 12(360, 480, 3)(3, 227, 227) 1&lt;matplotlib.image.AxesImage at 0x147e865f8&gt; output_10_2 开始分类12345678910111213# 将图像数据拷贝到为net分配的内存中# type(net.blobs['data'] -- &lt;class 'caffe._caffe.Blob'&gt;net.blobs['data'].data[...] = transformed_image# 执行分类# 前向传播，进行分类, 跑一遍网络，默认结果为最后一层的blob（也可以指定某一中间层），赋给outputoutput = net.forward()# output['prob']矩阵的维度是(50, 1000)# 取batch中第一张图像的概率值 (1000,)output_prob = output['prob'][0]# 打印概率最大的类别代号，argmax()函数是求取矩阵中最大元素的索引print('predicted class is:', output_prob.argmax()) 1predicted class is: 281 验证结果是否正确net 输出是一个概率向量，最可能的类别是第281个类别. 但是结果是否正确呢，需要查看一下ImageNet的标签… 12345678910# 加载ImageNet标签，如果不存在，则会自动下载labels_file = caffe_root + 'data/ilsvrc12/synset_words.txt'if not os.path.exists(labels_file): # !../data/ilsvrc12/get_ilsvrc_aux.sh print('not found')# 读取纯文本数据，三个参数分别是文件地址、数据类型和数据分隔符，保存为字典格式 labels = np.loadtxt(labels_file, str, delimiter='\\t')print('output label:', labels[output_prob.argmax()]) 1output label: n02123045 tabby, tabby cat “Tabby cat”是正确的，我们再来看下其它几个置信的较高的结果。 其他结果1234567# 从softmax output可查看置信度最高的五个结果# reverse逆序排列，取前五个最大值top_inds = output_prob.argsort()[::-1][:5]print('probabilites and labels: ')for prob, label in zip(output_prob[top_inds], labels[top_inds]): print(prob, label) 123456probabilites and labels: 0.312447 n02123045 tabby, tabby cat0.23797 n02123159 tiger cat0.123878 n02124075 Egyptian cat0.100752 n02119022 red fox, Vulpes vulpes0.0709571 n02127052 lynx, catamount 我们可以看出，较低置信度的结构也是合理的 GPU 模式加速(Mac无)1%timeit net.forward() 11.39 s ± 63.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) Mac 没有NVIDA的GPU, 故而不进行GPU加速, 加速的的确快 123456789'''# gpu模式下跑一次# 使用第一块显卡caffe.set_device(0)# 设为gpu模式caffe.set_mode_gpu()net.forward()&amp;timeit net.forward()''' 1&apos;\\n# gpu模式下跑一次\\n# 使用第一块显卡\\ncaffe.set_device(0)\\n# 设为gpu模式\\ncaffe.set_mode_gpu()\\nnet.forward()\\n&amp;timeit net.forward()\\n&apos; 测试中间输出结果 网络net不单单是一个黑盒子。我们接下来看看该模型的一些参数和一些中间输出。 中间层的可视化首先，我们来看下如何读取网络的结构(每层的名字以及相应层的参数) 每一层的结构四部分(batch_size, channel_dim, height, width) net.blobs 类型是 OrderedDict 1234# 对于每一层，显示输出维度# type(net.blobs) &lt;class 'collections.OrderedDict'&gt;for layer_name, blob in net.blobs.items(): print(layer_name, '\\t', str(blob.data.shape)) 123456789101112131415data (50, 3, 227, 227)conv1 (50, 96, 55, 55)pool1 (50, 96, 27, 27)norm1 (50, 96, 27, 27)conv2 (50, 256, 27, 27)pool2 (50, 256, 13, 13)norm2 (50, 256, 13, 13)conv3 (50, 384, 13, 13)conv4 (50, 384, 13, 13)conv5 (50, 256, 13, 13)pool5 (50, 256, 6, 6)fc6 (50, 4096)fc7 (50, 4096)fc8 (50, 1000)prob (50, 1000) 参数的维数我们来看下参数的维数. 参数是OrderdDict类型，我们根据索引来访问参数, net.params[0]:权值表示weights,net.params[1]:表示偏移量biases 权值参数的维度表示是(output_channels, input_channels, filter_height, filter_width)偏移量参数的维度表示(output_channels,) 一维 123# 循环打印参数名称，权值参数和偏移量参数的维度for layer_name, param in net.params.items(): print(layer_name, '\\t', str(param[0].data.shape), str(param[1].data.shape)) 12345678conv1 (96, 3, 11, 11) (96,)conv2 (256, 48, 5, 5) (256,)conv3 (384, 256, 3, 3) (384,)conv4 (384, 192, 3, 3) (384,)conv5 (256, 192, 3, 3) (256,)fc6 (4096, 9216) (4096,)fc7 (4096, 4096) (4096,)fc8 (1000, 4096) (1000,) 辅助函数 helper这里要将四维数据进行特征可视化，需要一个定义辅助函数： 12345678910111213141516171819202122232425262728293031323334353637def vis_square(data): ''' 输入一个形如：(n, height, width) or (n, height, width, 3)的数组， 并对每一个形如(height,width)的特征进行可视化到一个大小近似的网格中sqrt(n) by sqrt(n) ''' # 数据正则化 data = (data - data.min()) / (data.max() - data.min()) ''' 将滤波器的核转变为正方形 此处目的是将一个个滤波器按照正方形的样子排列, 先对shape[0]也就是滤波器数量取平方根，然后取大于等于该结果的正整数 ceil 向上取整 比如40个卷积核，则需要7*7的正方形格子（虽然填不满） ''' n = int(np.ceil(np.sqrt(data.shape[0]))) padding = ( ((0, n**2 - data.shape[0]), (0, 1), (0, 1)) # 在相邻的卷积核之间加入空白 + ((0, 0),) *(data.ndim - 3) # 不填充最后一维 ) # 每张小图片向周围扩展一个白色像素 data = np.pad(data, padding, mode='constant', constant_values=1) ''' pad函数声明：pad(array, pad_width, mode, **kwargs)，作用是把list在原维度上进行扩展; pad_width是扩充参数，例如参数((3,2),(2,3)); 其中(3，2)为水平方向上，上面加3行，下面加2行； (2，3)为垂直方向上，上面加2行，下面加3行; (1, 1)相当于常数, 各个方向都填充 constant是常数填充的意思。 ''' # 将卷积核平铺成图片 data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3) + tuple(range(4, data.ndim+1))) data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:]) plt.imshow(data) plt.axis('off') 卷积层(conv1)的输出特征 首先，我们来看下第一个卷积层(conv1)的输出特征 123# 参数为一个[weights, biases]的列表filters = net.params['conv1'][0].datavis_square(np.transpose(filters, (0, 2, 3, 1))) output_28_0 The first layer output, conv1 (rectified responses of the filters above, first 36 only) conv1 的blob数据123# 选取‘conv1’的blob数据，只选择前面36张图片feat = net.blobs['conv1'].data[0, :36]vis_square(feat) output_30_0 pool512feat = net.blobs['pool5'].data[0]vis_square(feat) output_32_0 fc6第一个完全连接层，fc6;接下来，我们将显示输出结果(&gt;0的值)及直方图. 123456789# 选取fc6的输出数据，这是一个4096维的向量feat = net.blobs['fc6'].data[0] # 创建2行1列的子图，现在是第1个子图plt.subplot(2, 1, 1)# 平铺向量，图像显示其每一个值plt.plot(feat.flat)plt.subplot(2, 1, 2)# 做直方图，总共100根条形_ = plt.hist(feat.flat[feat.flat &gt; 0], bins=100) output_34_0 最终的概率输出 prob注意强大的预测集群; 标签按照语义排序。 如上所示，顶部峰值对应于顶部预测标签。 123feat = net.blobs['prob'].data[0]plt.figure(figsize=(15, 3))plt.plot(feat.flat) 1[&lt;matplotlib.lines.Line2D at 0x148b944e0&gt;] output_36_1 测试自己的图片123456789101112131415161718192021222324'''# 下载图像# 图像URL地址my_image_url = \"\"# 在线下载图片!wget -O image.jpg $my_image_url '''# 变换图像并将其拷贝到网络image = caffe.io.load_image('../images.jpeg')net.blobs['data'].data[...] = transformer.preprocess('data', image) # 预测分类结果net.forward()# 获取输出概率值output_prob = net.blobs['prob'].data[0]# 将softmax的输出结果按照从大到小排序，并取前5名top_inds = output_prob.argsort()[::-1][:5]plt.imshow(image)print('probabilities and labels:')for prob, label in zip(output_prob[top_inds], labels[top_inds]): print(prob, '\\t', label) 123456probabilities and labels:0.936924 n02099712 Labrador retriever0.0573292 n02099601 golden retriever0.00213249 n02088364 beagle0.00084234 n02104029 kuvasz0.000632635 n02111500 Great Pyrenees output_38_1","categories":[{"name":"feature_extraction","slug":"feature-extraction","permalink":"http://yaoyirong.cn/categories/feature-extraction/"}],"tags":[{"name":"Image processing","slug":"Image-processing","permalink":"http://yaoyirong.cn/tags/Image-processing/"}]},{"title":"Image processing - Feature extraction","slug":"图像特征识别文字性描述","date":"2017-12-26T07:47:00.000Z","updated":"2017-12-26T07:48:17.000Z","comments":true,"path":"2017/12/26/图像特征识别文字性描述/","link":"","permalink":"http://yaoyirong.cn/2017/12/26/图像特征识别文字性描述/","excerpt":"纯文字描述","text":"纯文字描述 [TOC] 图像特征识别 调研 针对图像, 视频, 音频特征提取的工具和算法(方法) 图像特征概述属于图像分析的范畴, 也是图像识别的开始。如何从图像中提取有用的数据或信息，得到图像的“非图像” 的表示或描述，如数值、向量和符号等(特征提取). “非图像”的表示或描述就是特征(数值或向量形式)。 图像特征对于图像而言， 每一幅图像都具有能够区别于其他类图像的自身特征，有些是可以直观地感受到的自然特征，如亮度、边缘、纹理和色彩等；有些则是需要通过变换或处理才能得到的， 如矩、直方图以及主成份等. 特征向量将某一类对象的多个或多种特性组合在一起, 形成一个特征向量来代表该类对象，如果只有单个数值特征，则特征向量为一个一维向量，如果是n个特性的组合，则为一个n维特征向量。该类特征向量常常作为识别系统的输入。 一个n维特征就是位于n维空间中的点，而识别分类的任务就是找到对这个n维空间的一种划分。 特征提取的一般原则图像识别实际上是一个分类的过程，要求选取的特征不仅要能够很好地描述图像，更重要的是还要能够很好地区分不同类别的图像。 在同类图像之间差异较小（较小的类内距），在不同类别的图像之间差异较大（较大的类间距）的图像特征 —- 最具有区分能力(most discriminative)的特征。在特征提取中先验知识扮演着重要的角色， 如何依靠先验知识来帮助我们选择特征也是后面将持续关注的问题。 特征的评价标准特征提取应具体问题具体分析, 还是有一些可供遵循的普遍原则: 特征应当容易提取 选取的特征应对噪声和不相关转换不敏感． 比如: 需要得到对几何失真变形等转换不敏感的描绘子， 从而得到旋转不变， 或是投影失真不变的特征 应试图寻找最具区分能力的特征． 基本统计特征 常用的基本统计特征，包括一些简单的区域描绘子, 直方图及其统计特征以及灰度共现矩阵等 简单的区域描绘子及其Matlab实现在经过图像分割得到各种我们感兴趣的区域之后patch_tree = img[7:150, 300:450], 利用一些简单区域描绘子作为代表该区域的特征。通常将这些区域特征组合成特征向量以供分类使用。 常用的简单区域描绘子如下: 周长: 区域边界的长度， 即位于区域边界上的像素数目 面积: 区域中的像素总数． 致密性: (周长） 2/面积． 区域的质心 灰度均值: 区域中所有像素的平均值． 灰度中值: 区域中所有像素的排序中值． 包含区域的最小矩形． 最小或最大灰度级 大于或小于均值的像素数 欧拉数: 区域中的对象数减去这些对象的孔洞数 MATLAB 使用 regionprops 计算区域描绘子, 其原型 D = regionprops(L,properties) L是一个标记矩阵, properties可以是一个用逗号分割的字符串列表，常用取值如下: ‘Area’ : 区域中的像素总数． ‘BoundingBox’: 包含区域的最小矩形. 1x4向量(矩形左上角x坐标, 矩形左上角y坐标, x方向长度, y方向长度) Centroid: 区域的质心. 1x2向量: [质心x坐标, 质心y坐标] ConvexHull: 含区域的最小凸多边形. Px2矩阵: 每一行包含多边形p个顶点之一的x和y坐标 EquivDiameter: 和区域有着相同面积的圆的直径 EulerNumber: 欧拉数, 区域中的对象数减去这些对象的孔洞数 直方图及其统计特征 (一维)纹理是图像固有的特征之一，是灰度（对彩色图像而言是颜色）在空间以一定的形式变换而产生的图案（模式），有时具有一定的周期性。直方图正是描述图像中像素灰度级分布的有力工具，可以用直方图或其统计特征作为图像纹理特征。 直方图本身就是一个向量，向量的维数是直方图统计的灰度级数，因此可以直接以此向量作为代表图像纹理的样本特征向量，从而交给分类器处理，对于LBP直方图就常常这样处理）；另一种思路是进一步从直方图中提取出能够很好地描述直方图的统计特征，将直方图的这些统计特征组合成为样本特征向量， 这样做可以大大降低特征向量的维数。 直方图的常用统计特征:均值, 标准方差, 平滑度, 三阶矩, 一致性, 熵一个由均值、标准差、平滑度和熵组合而成的特征向量如：v = (m,a, R, e)。 依靠直方图及其统计特征来作为分类特征时需要特别注意应认识到直方图及其统计特征是一种区分能力相对较弱的特征，这主要因为直方图属于一阶统计特征，而它们的一阶统计特征是无法反映纹理结构的变化的。直方图与纹理的对应关系并不是一对一的：首先，不同的纹理可能具有相同或相似的直方图; 其次，即便是两个不同的直方m.也可能具有相同的统计特 征如均值、标准差等。 直方图常用统计特征 直方图常用统计特征接上 灰度共现矩阵 (二维)灰度直方图是一种描述单个像素灰度分布的一阶统计量；而灰度共现矩阵描述的则是具有某种空间位置关系的两个像素的联合分布, 可以看成是两个像素灰度对的联合直方图，是种二阶统计量。 纹理是由灰度分布在空间位置上反复交替变化得来的, 因此在图像中具有某种空间位置关系的两个像素之间会存在一定的灰度关系, 这种关系被称为图像灰度的空间相关特性.灰度共现矩阵可以比较好的表现灰度的空间相关性 x y 单位 灰度级是L 则为L x L 方阵, 其中每个元素 Pd(i, j)Pd(i, j) 为具有空间位置关系 d = (Dx, Dy), 并且灰度分别为 i 和 j 的两个像素出现的次数或者是概率(归一化) 常用的空间位置关系: 水平(±Dx,0), 竖直(0, ±Dy) 正45°(Dx,-Dy), (-Dx, Dy)负45°(Dx, Dy), (-Dx, -Dy) 当灰度级L比较大时它将是一个庞大的方阵。如对于一般的256灰度图，凡就是一个256X256的矩阵，共2^16个元素。如此庞大的矩阵将使后续的计算量剧增。因此普通灰度图像通常要经过处理以减少灰度级数，而后再计算灰度共现矩阵。可以通过分析纹理图像的直方图，在尽量不影响纹理质量的情况下．通过适当的灰度变换来达到灰度级压缩的目的。 特征降维维度灾难 维度灾难 最大值时，分类器的性能不是得到改善，而是退化。在低维空间中计算和分类都将变 得简单很多，训练（教授分类器如何区分不同类样本的过程，详见第11章）所需的样本数目也会大大降低。通过选择好的特征，摒弃坏的特征(10.3.2特征选择），将有助于分类器性能的提升；在通过组合特征降维时，在绝大多数情况下，丢弃某些特征所损失的信息通过在低 维空间中更加精确的映射(10.3.3特征抽取）可以得到补偿。 降维的两个方法: 特征选择 选择全部特征的一个子集作为特征向量 特征抽取 通过已有特征的组合建立一个新的特征子集 如主成份分析方法(principa1component analysis, PCA) 原特征的线性组合 主成分分析(Princjpal Component Analysis, PCA)特征抽取是指通过已有特征的组合（变换）建立一个新的特征子集。在众多的组合方法当中，线性组合（变换）因其计算简单且便于解析分析的特点而显得颇具吸引力。PCA的实质就是在尽可能好地代表原始数据的前提下，通过线性变换将高维空间中的样本数据投影到低维空间中. 图片特征提取图像处理的基础就是要进行特征点的提取，feature(interest points) detect 的方法也在不断的进步，边检测，角点检测，直线检测，圆检测，SIFT特征点检测，同时描述符也在发展，为了匹配的高效，逐渐从高维特征向量到二进制向量… 通过像素值提取特征比如识别光学字符, 0-9; scikit-learn的digits数字集包括至少1700种0-9的手写数字图像。每个图像都有8x8像像素构成。每个像素的值是0-16，白色是0，黑色是16。将8x8矩阵转换成64维向量来创建一个特征向量： 1234567import numpy as npimport matplotlib.pyplot as pltfrom sklearn import datasetsdigits = datasets.load_digits()# reshape的返回值都是 narray, 如果参数只有-1, 就是一维, 后面还加了64, 就是二维(1, 64)print(digits.image[0].reshape(-1, 64) 可以有效的处理一些基本任务，比如识别手写字母等。但是，记录每个像素的数值在大图像处理时不太好用。特征向量的维度灾难; 放缩，旋转或变换之后可能不对,缺乏稳定性; 对图像的亮度也十分敏感.现代计算机视觉应用通常手工实现特征提取，或者用深度学习自动化解决无监督问题。 对感兴趣的点进行特征提取有信息量的属性，称为兴趣点（points of interest); 由丰富的纹理包围，基本可以重建图像。边缘（edges）和角点（corners）是两种常用的兴趣点类型。边是像素快速变化的分界线（boundary),这种提取方式更紧凑，而且当图片的亮度发生统一变化时，这些兴趣点依然存在。 Harris角点检测scikit-image(skimage)库抽取图片的兴趣点corners = corner_peak(corner_harris(image), min_distance=2)corner_x, corner_y = zip(*corners) SIFT 特征提取主要的思想是每个检测到的特征点都伴随对应的尺度因子, 具有尺度，旋转，仿射，视角，光照不变性。引入兴趣点提取方法，通过SIFT和SURF进行优化。 尺度不变特征转换（Scale-Invariant Feature Transform，SIFT), 相比前面使用的方法，SIFT对图像的尺寸，旋转，亮度变化更不敏感。每个SIFT特征都是一个描述图片上某个区域边缘和角点的向量. 和兴趣点不同，SIFT还可以获取每个兴趣点和它周围点的综合信息 SURF 特征抽取加速稳健特征（Speeded-Up Robust Features，SURF), 是另一个抽取图像兴趣点的方法,其特征向量对图像的尺寸，旋转，亮度变化是不变的。其特征向量对图像的尺寸，旋转，亮度变化是不变的。和兴趣点抽取类似，抽取SURF只是机器学习中创建特征向量的第一步。训练集的每个实例都会抽取不同的SURF。第六章的，K-Means聚类，我们会介绍聚类方法抽取SURF来学习特征，可以作为一种图像分类方法.mahotas(计算机视觉和图像处理 Python 库)包含算法: 分水岭。 凸点计算。 击中/击不中，细化算法。 泽尼克＆Haralick，枸杞多糖，和TAS的功能。 基于freeimage的numpy图像加载（需要安装freeimage库）。 加速的鲁棒特征（SURF）等。 阈值。 卷积。 Sobel边缘检测。 多边形绘制 距离变换 特征计算 样条插值 123456import mahotas as mhfrom mahotas.features import surfimage = mh.imread('mandrill.jpeg', as_grey=True)print('第一个SURF描述符: \\n&#123;&#125;\\n'.format(surf.surf(image)[0]))print('抽取了%s个SURF描述符' % len(surf.surf(image))) 数据标准化确保解释变量的数据都是同一量级，均值为0的标准化数据。许多评估方法在处理标准化数据集时可以获得更好的效果。标准化数据均值为0，单位方差(Unit Variance)。均值为0的解释变量是关于原点对称的，特征向量的单位方差表示其特征值全身统一单位，统一量级的数据。 例如，假设特征向量由两个解释变量构成，第一个变量值范围[0,1]，第二个变量值范围[0,1000000]，这时就要把第二个变量的值调整为[0,1]，这样才能保证数据是单位方差。如果变量特征值的量级比其他特征值的方差还大，这个特征值就会主导学习算法的方向，导致其他变量的影响被忽略。有些机器学习算法会在数据不标准时引入很小的优化参数值。 解释变量的值可以通过正态分布进行标准化，减去均值后除以标准差。scikit-learn的scale函数可以实现： 12345678from sklearn import preprocessingimport numpy as npX = np.array([ [0., 0., 5., 13., 9., 1.], [0., 0., 13., 15., 10., 15.], [0., 3., 15., 2., 0., 11.]])print(preprocessing.scale(X))","categories":[{"name":"Feature extraction","slug":"Feature-extraction","permalink":"http://yaoyirong.cn/categories/Feature-extraction/"}],"tags":[{"name":"Image processing","slug":"Image-processing","permalink":"http://yaoyirong.cn/tags/Image-processing/"}]},{"title":"Image Feature extraction example","slug":"图像特征提取实例","date":"2017-12-26T07:24:00.000Z","updated":"2017-12-26T07:49:02.000Z","comments":true,"path":"2017/12/26/图像特征提取实例/","link":"","permalink":"http://yaoyirong.cn/2017/12/26/图像特征提取实例/","excerpt":"关于图像特征提取的几个小栗子:","text":"关于图像特征提取的几个小栗子: Feature_extraction_ex1以下是几个关于图像特征提取的小栗子: 识别手写数字关于的手写数字识别的OCR问题，通过图像的像素矩阵扁平化来学习手写数字特征。(非常耗费资源)scikit-learn的digits数字集包括至少1700种0-9的手写数字图像。每个图像都有8x8像像素构成。每个像素的值是0-16，白色是0，黑色是16。如下图所示： 12345678910111213%matplotlib inlineimport numpy as npimport matplotlib.pyplot as pltfrom sklearn import datasetsdigits = datasets.load_digits()print('Digits: ', digits.target[0])# 8x8像像素构成。每个像素的值是0-16print(digits.images[0].reshape(-1))plt.figure()plt.axis('off')plt.imshow(digits.images[0], cmap=plt.cm.gray_r, interpolation='nearest')plt.show() 123456Digits: 0[ 0. 0. 5. 13. 9. 1. 0. 0. 0. 0. 13. 15. 10. 15. 5. 0. 0. 3. 15. 2. 0. 11. 8. 0. 0. 4. 12. 0. 0. 8. 8. 0. 0. 5. 8. 0. 0. 9. 8. 0. 0. 4. 11. 0. 1. 12. 7. 0. 0. 2. 14. 5. 10. 12. 0. 0. 0. 0. 6. 13. 10. 0. 0. 0.] output_3_1 scikit-image库抽取下图的兴趣点：12345678910111213141516171819202122from skimage.feature import corner_harris, corner_peaksfrom skimage.color import rgb2grayimport skimage.io as iofrom skimage.exposure import equalize_histdef show_corners(corners, image): fig = plt.figure() plt.gray() plt.imshow(image) y_corner, x_corner = zip(*corners) plt.plot(x_corner, y_corner, 'or') plt.xlim(0, image.shape[1]) plt.ylim(image.shape[0], 0) fig.set_size_inches(np.array(fig.get_size_inches()) * 1.5) plt.show() mandrill = io.imread('mandrill.jpeg')# 直方图均衡化mandrill = equalize_hist(rgb2gray(mandrill))# harris 角点检测corners = corner_peaks(corner_harris(mandrill), min_distance=2)show_corners(corners, mandrill) output_5_0 用mahotas库来应用SURF方法处理下面的图片引入兴趣点提取方法，通过SIFT和SURF进行优化。Mahotas 是计算机视觉和图像处理 Python 库。它包含大量图像处理算法，C++实现形式，提高了性能。完全基于 numpy 的数组作为它的数据类型，有一个非常干净的Python 算法接口。 123456import mahotas as mhfrom mahotas.features import surfimage = mh.imread('mandrill.jpeg', as_grey=True)print('第一个SURF描述符: \\n&#123;&#125;\\n'.format(surf.surf(image)[0]) )print('抽取了%s个SURF描述符' % len(surf.surf(image))) 123456789101112131415161718192021第一个SURF描述符: [ 4.88720413e+01 1.24800860e+02 1.93404140e+00 4.69215836e+02 1.00000000e+00 -1.89817090e+00 -9.16485838e-04 4.15829239e-04 1.62917514e-03 4.50508786e-03 2.16965117e-03 -1.54517486e-02 1.19871085e-02 2.27849309e-02 -4.96764299e-03 -2.30153165e-03 1.24213881e-02 9.73558091e-03 1.01124633e-03 9.33727930e-05 1.50382252e-03 4.28077728e-04 4.12196201e-04 2.63136511e-03 1.47364160e-02 2.57975865e-02 2.13572331e-01 1.36266382e-03 3.25871891e-01 2.83610749e-01 3.01434634e-01 -8.35839666e-02 4.67635391e-01 1.61936283e-01 6.27416198e-03 -4.75608140e-03 6.38395923e-02 2.88552374e-02 4.36529968e-03 1.22763937e-02 4.28753085e-02 2.97861362e-02 -1.65692866e-01 -3.00393938e-02 3.07613402e-01 2.59695442e-01 -1.77392860e-01 -8.42926232e-02 3.22241967e-01 2.68252184e-01 1.22916451e-02 -2.17223444e-03 3.34867018e-02 7.92410354e-03 -3.77849533e-04 -3.75246341e-03 5.13159418e-03 4.26743799e-03 2.47661732e-02 -9.75867100e-03 3.38654071e-02 2.22350151e-02 1.25097158e-02 -5.13047076e-03 6.62972574e-02 3.98549144e-02 7.56108237e-03 4.19496163e-04 1.53695063e-02 5.35408794e-03]抽取了222个SURF描述符 数据标准化scikit-learn的scale函数实现: 123456789from sklearn import preprocessingimport numpy as npX = np.array([ [0., 0., 5., 13., 9., 1.], [0., 0., 13., 15., 10., 15.], [0., 3., 15., 2., 0., 11.]])print(preprocessing.scale(X)) 123[[ 0. -0.70710678 -1.38873015 0.52489066 0.59299945 -1.35873244] [ 0. -0.70710678 0.46291005 0.87481777 0.81537425 1.01904933] [ 0. 1.41421356 0.9258201 -1.39970842 -1.4083737 0.33968311]] OpenCV SURF() 特征提取KeyPoint 数据结构 angle：角度，表示关键点的方向 class_id：当要对图片进行分类时，我们可以用class_id对每个特征点进行区分，未设定时为-1 octave：代表是从金字塔哪一层提取的得到的数据。 pt：关键点点的坐标 —-这次用到的 response：响应程度，代表该点强壮大小; response代表着该关键点how good，更确切的说，是该点角点的程度。 size：该点直径的大小 注意一个问题：keypoint只是保存了opencv的sift库检测到的特征点的一些基本信息，也就上面所说的这些，但sift所提取出来的特征向量其实不是在这个里面，特征向量通过SiftDescriptorExtractor 提取，结果放在一个Mat的数据结构中。这个数据结构才真正保存了该特征点所对应的特征向量。具体见后文对SiftDescriptorExtractor 所生成的对象的详解。 123456789101112131415161718192021222324252627282930313233343536import cv2image = cv2.imread('surf_exm.jpg')image2 = image.copy()#cv2.imshow('original', image)#cv2.waitKey()# 下采样# 高斯图像金字塔#im_lowers = cv2.pyrDown(image) #cv2.imshow('imlowers', im_lowers)# 检测特征点# 调用SIFT# opencv将SIFT等算法整合到xfeatures2d集合里面了。变更后写法如下：s = cv2.xfeatures2d.SIFT_create()keypoints = s.detect(image)# type(keypoints) &lt;class 'list'&gt;# len(keypoints) 6212# 显示特征点for k in keypoints: cv2.circle(image, (int(k.pt[0]), int(k.pt[1])), 1, (0, 255, 0), -1) s1 = cv2.xfeatures2d.SURF_create()keypoints2 = s1.detect(image)for kk in keypoints2: # circle 函数 (指针, 圆心坐标, 圆半径, 颜色, thickness正数表示组成圆的线条的粗细程度;否则，表示圆是否被填充 cv2.circle(image2, (int(kk.pt[0]), int(kk.pt[1])), 1, (0, 255, 0), -1) #cv2.circle(image2, (int(kk.pt[0]), int(kk.pt[1])), int(k.size), (0, 255, 0), -1)cv2.imwirte('SIFT_features.jpg',image)cv2.imwrite('SURF_features.jpg',image2)cv2.waitKey() 原图: surf_exm SIFT: SIFT_features SURF: SURF_features","categories":[{"name":"Feature extraction","slug":"Feature-extraction","permalink":"http://yaoyirong.cn/categories/Feature-extraction/"}],"tags":[{"name":"Image processing","slug":"Image-processing","permalink":"http://yaoyirong.cn/tags/Image-processing/"}]},{"title":"OpenCV_python 使用","slug":"OpenCV_python使用","date":"2017-12-22T04:30:13.000Z","updated":"2017-12-23T09:01:56.000Z","comments":true,"path":"2017/12/22/OpenCV_python使用/","link":"","permalink":"http://yaoyirong.cn/2017/12/22/OpenCV_python使用/","excerpt":"python-OpenCV","text":"python-OpenCV Python - OpenCV介绍和深度学习数据处理阶段最相关的基础使用，并完成4个有趣实用的小例子： 延时摄影小程序 视频中截屏采样的小程序 图片数据增加（data augmentation）的小工具 物体检测框标注小工具 OpenCV 简介OpenCV是计算机视觉领域应用最广泛的开源工具包，基于C/C++，支持Linux/Windows/MacOS/Android/iOS，并提供了Python，Matlab和Java等语言的接口. OpenCV的结构 和Python一样，当前的OpenCV也有两个大版本，OpenCV2和OpenCV3。相比OpenCV2，OpenCV3提供了更强的功能和更多方便的特性。不过考虑到和深度学习框架的兼容性，以及上手安装的难度，这部分先以2为主进行介绍。从使用的角度来看，和OpenCV2相比，OpenCV3的主要变化是更多的功能和更细化的模块划分。 根据功能和需求的不同，OpenCV中的函数接口大体可以分为如下部分： core：核心模块，主要包含了OpenCV中最基本的结构（矩阵，点线和形状等），以及相关的基础运算/操作。 imgproc：图像处理模块，包含和图像相关的基础功能（滤波，梯度，改变大小等），以及一些衍生的高级功能（图像分割，直方图，形态分析和边缘/直线提取等). highgui：提供了用户界面和文件读取的基本函数，比如图像显示窗口的生成和控制，图像/视频文件的IO等。 如果不考虑视频应用，以上三个就是最核心和常用的模块了. 针对视频和一些特别的视觉应用，OpenCV也提供了强劲的支持： video：用于视频分析的常用功能，比如光流法（Optical Flow）和目标跟踪等。 calib3d：三维重建，立体视觉和相机标定等的相关功能。 features2d：二维特征相关的功能，主要是一些不受专利保护的，商业友好的特征点检测和匹配等功能，比如ORB特征。 object：目标检测模块，包含级联分类和Latent SVM ml：机器学习算法模块，包含一些视觉中最常用的传统机器学习算法。 flann：最近邻算法库，Fast Library for Approximate Nearest Neighbors，用于在多维空间进行聚类和检索，经常和关键点匹配搭配使用。 gpu：包含了一些gpu加速的接口，底层的加速是CUDA实现。 photo：计算摄像学（Computational Photography）相关的接口，当然这只是个名字，其实只有图像修复和降噪而已。 stitching：图像拼接模块，有了它可以自己生成全景照片。 nonfree：受到专利保护的一些算法，其实就是SIFT和SURF。 contrib：一些实验性质的算法，考虑在未来版本中加入的。 legacy：字面是遗产，意思就是废弃的一些接口，保留是考虑到向下兼容。 ocl：利用OpenCL并行加速的一些接口。 superres：超分辨率模块，其实就是BTV-L1（Biliteral Total Variation – L1 regularization）算法 viz：基础的3D渲染模块，其实底层就是著名的3D工具包VTK（Visualization Toolkit）。 基本使用图像就是一个矩阵，在OpenCV for Python中，图像就是NumPy中的数组, 图像使用NumPy数组的属性来表示图像的尺寸和通道信息 读取图像读图像用cv2.imread()，可以按照不同模式读取，一般最常用到的是读取单通道灰度图，或者直接默认读取多通道。存图像用cv2.imwrite()，注意存的时候是没有单通道这一说的, 根据保存文件名的后缀和当前的array维度，OpenCV自动判断存的通道，另外压缩格式还可以指定存储质量. 123456789101112# -----------------------------# 读取图像# -----------------------------import cv2img = cv2.imread('little_white_dog.jpeg')cv2.namedWindow('Image')cv2.imshow('Image', img)cv2.waitKey(0)cv2.destroyAllWindows()print(img.shape) 123456789101112131415161718192021222324import cv2import numpy as npimg = cv2.imread('figure.jpg')px = img[100,100] # [128 122 153]print( px) blue = img[100,100,0] print(blue)# 以上这种读取像素值的方式非常缓慢。推荐使用Numpy的函数——array.item()和array.itemset()来访问# accessing red value red = img.item(100, 100, 2)print(red)# modifying RED value img.itemset((100,100,2), 10)img.item(100, 100, 2)# 获取图像的属性——行数、列数、通道数、图像的数据类型以及像素点的数量等img.shape# 获得像素点的数量img.sizeimg.dtype 12345678910111213141516171819import cv2import numpy as npcolor_img = cv2.imread('test_400x600.jpeg')print(color_img.shape)# 直接读取单通道gray_img = cv2.imread('test_400x600.jpeg', cv2.IMREAD_GRAYSCALE)print(gray_img.shape)# 把单通道图片保存后，再读取，仍然是3通道，相当于把单通道值复制到3个通道保存cv2.imwrite('test_grayscale.jpeg', gray_img)reload_grascale = cv2.imread('test_grayscale.jpeg')print(reload_grascale.shape)# cv2.IMWRITE_JPEG_QUALITY指定jpg质量，范围0到100，默认95，越高画质越好，文件越大cv2.imwrite('test_imwrite.jpeg', color_img, (int(cv2.IMWRITE_JPEG_QUALITY), 80))cv2.imwrite('test_imwrite.png', color_img, (int(cv2.IMWRITE_PNG_COMPRESSION), 5)) 创建/复制图像12345678910111213141516171819202122232425262728293031323334# -----------------------------# 创建/复制图像# 图像就是一个矩阵，在OpenCV for Python中，图像就是NumPy中的数组# 如果要创建图像，需要使用numpy的函数, 图像使用NumPy数组的属性来表示图像的尺寸和通道信息# -----------------------------import cv2import numpy as npimg = cv2.imread('little_white_dog.jpeg')emptyImage = np.zeros(img.shape, np.uint8)emptyImage2 = img.copy()emptyImage3 = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)# emptyImage3[...] = 0cv2.imshow('EmptyImage', emptyImage)cv2.imshow('Image', img)cv2.imshow('EmptyImage2', emptyImage2)cv2.imshow('EmptyImage3', emptyImage3)# 第三个参数针对特定的格式： 对于JPEG，其表示的是图像的质量，# 用0-100的整数表示，默认为95。 注意，cv2.IMWRITE_JPEG_QUALITY类型为Long，必须转换成int。下面是以不同质量存储的两幅图：cv2.imwrite('little_white_dog2.jpg', img, [int(cv2.IMWRITE_JPEG_QUALITY), 5])cv2.imwrite('little_white_dog3.jpg', img, [int(cv2.IMWRITE_JPEG_QUALITY), 100])# 对于PNG，第三个参数表示的是压缩级别。cv2.IMWRITE_PNG_COMPRESSION，从0到9,压缩级别越高，图像尺寸越小。默认级别为3：cv2.imwrite('little_white_dog2.png', img, [int(cv2.IMWRITE_PNG_COMPRESSION), 0])cv2.imwrite('little_white_dog3.png', img, [int(cv2.IMWRITE_PNG_COMPRESSION), 9])cv2.waitKey(0)cv2.destroyAllWindows() 123456789# numpy 全复制 不是镜像import cv2import numpy as npimg = cv2.imread('little_white_dog.jpeg')c = np.zeros(img.shape, dtype=img.dtype)c[:, :, :] = img[:, :, :]c is img # falsec.base is img # false 图像元素的访问与C++不同，在Python中灰度图的img.ndim = 2，而C++中灰度图图像的通道数img.channel() =1 这里使用了numpy的随机数，Python自身也有一个随机数生成函数。这里只是一种习惯，np.random模块中拥有更多的方法，而Python自带的random只是一个轻量级的模块。不过需要注意的是np.random.seed()不是线程安全的，而Python自带的random.seed()是线程安全的。如果使用随机数时需要用到多线程，建议使用Python自带的random()和random.seed()，或者构建一个本地的np.random.Random类的实例。 123456789101112131415161718192021222324252627# -----------------------------# 图像元素的访问# 像素的访问和访问numpy中ndarray的方法完全一样# 下面通过对图像添加人工的椒盐现象来进一步说明OpenCV Python中需要注意的一些问题# -----------------------------import cv2import numpy as npdef salt(img, n): for k in range(n): j = int(np.random.random() * img.shape[0]) i = int(np.random.random() * img.shape[1]) if img.ndim == 2: img[j, i] = 255 elif img.ndim ==3: img[j, i, 0] = 255 img[j, i, 1] = 255 img[j, i, 2] = 255 return img if __name__ == '__main__': img = cv2.imread('iPhone.png') saltImage = salt(img, 500) cv2.imshow('Salt', saltImage) cv2.waitKey(0) cv2.destoryAllWindows() 缩放，裁剪和补边缩放通过cv2.resize()实现(指定大小的格式是(宽度,高度))，裁剪则是利用array自身的下标截取实现，此外OpenCV还可以给图像补边，这样能对一幅图像的形状和感兴趣区域实现各种操作。 下面的例子中读取一幅400×600分辨率的图片，并执行一些基础的操作： 1234567891011121314151617181920212223242526272829303132# 读取一张600x375分辨率的图像img = cv2.imread('figure.jpg')# 缩放成200x200的方形图像img_200x200 = cv2.resize(img, (200, 200))# 不直接指定缩放后大小，通过fx和fy指定缩放比例，0.5则长宽都为原来一半# 等效于img_200x300 = cv2.resize(img, (300, 200))，注意指定大小的格式是(宽度,高度)# 插值方法默认是cv2.INTER_LINEAR，这里指定为最近邻插值# fx、fy是沿x轴和y轴的缩放系数 和 dsize不能同时为0'''最优一个参数interpolation表示插值方式，有以下几种：INTER_NEAREST - 最近邻插值INTER_LINEAR - 线性插值（默认）INTER_AREA - 区域插值INTER_CUBIC - 三次样条插值INTER_LANCZOS4 - Lanczos插值'''img_200x300 = cv2.resize(img, (200, 300), fx=0.5, fy=0.5, interpolation=cv2.INTER_NEAREST)# 在上张图片的基础上，上下各贴50像素的黑边，生成300x300的图像img_300x300 = cv2.copyMakeBorder(img, 50, 50, 0, 0, cv2.BORDER_CONSTANT, value=(0, 0, 0))# 对照片中人脸的部分进行剪裁 高度区间和宽度区间 # patch_tree = img[20:150, -300:-10]patch_tree = img[7:150, 300:450]cv2.imwrite('cropped_tree.jpg', patch_tree)cv2.imwrite('resize_200x300.jpg', img_200x300)cv2.imwrite('bordered_300x300.jpg', img_300x300)cv2.imwrite('resize_200x200.jpg', img_200x200) 补边: bordered_300x300 裁剪: cropped_tree 色调，明暗，直方图和Gamma曲线除了区域，图像本身的属性操作也非常多，比如可以通过HSV空间对色调和明暗进行调节。(HSV分别是色调(Hue), 饱和度（Saturation）和明度（Value）。在HSV空间中进行调节就避免了直接在RGB空间中调节是还需要考虑三个通道的相关性. OpenCV中H的取值是[0, 180)，其他两个通道的取值都是[0, 256). 下面例子接着上面例子代码，通过HSV空间对图像进行调整： 12345678910111213141516171819202122232425import cv2img = cv2.imread('figure.jpg')# 通过cv2.cvtColor把图像从BGR转换到HSVimg_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)turn_green = img_hsv.copy()colorless_hsv = img_hsv.copy()darker_hsv = img_hsv.copy()# H空间中，绿色比黄色的值高一点，所以给每个像素+15，黄色的树叶就会变绿turn_green[:, :, 0] = (turn_green[:, :, 0]+15) % 180turn_green_img = cv2.cvtColor(turn_green, cv2.COLOR_HSV2BGR)cv2.imwrite('trun_green.jpg', turn_green_img)# 减小饱和度会让图像损失鲜艳，变得更灰colorless_hsv[:, :, 1] = 0.5 * colorless_hsv[:, :, 1]colorless_img = cv2.cvtColor(colorless_hsv, cv2.COLOR_HSV2BGR)cv2.imwrite('colorless.jpg', colorless_img)# 减小明度为原来一半darker_hsv[:, :, 2] = 0.5 * darker_hsv[:, :, 2]darker_img = cv2.cvtColor(darker_hsv, cv2.COLOR_HSV2BGR)cv2.imwrite('darker.jpg', darker_img) 更改色调 更改饱和度 更改亮度 直方图 无论是HSV还是RGB，我们都较难一眼就对像素中值的分布有细致的了解，这时候就需要直方图。如果直方图中的成分过于靠近0或者255，可能就出现了暗部细节不足或者亮部细节丢失的情况。 这个时候，一个常用方法是考虑用Gamma变换来提升暗部细节。Gamma变换是矫正相机直接成像和人眼感受图像差别的一种常用手段， 简单来说就是通过非线性变换让图像从对曝光强度的线性响应变得更接近人眼感受到的响应。 矫正前: darker 矫正后: img_corrected 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import numpy as npimport cv2import matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3D'''cv2.calcHist(images, channels, mask, histSize, ranges[, hist[, accumulate ]]) 返回hist其中第一个参数必须用方括号括起来。第二个参数是用于计算直方图的通道第三个参数是Mask，这里没有使用，所以用None。第四个参数是histSize，表示这个直方图分成多少份（即多少个直方柱）第五个参数是表示直方图中各个像素的值, [0.0, 256.0]表示直方图能表示像素值从0.0到256的像素。最后是两个可选参数，由于直方图作为函数结果返回了，所以第六个hist就没有意义了（待确定）最后一个accumulate是一个布尔值，用来表示直方图是否叠加。'''img = cv2.imread('darker.jpg')# 直方图 &lt;class 'numpy.ndarray'&gt; (256, 1)hist_b = cv2.calcHist([img], [0], None, [256], [0, 256])hist_g = cv2.calcHist([img], [1], None, [256], [0, 256])hist_r = cv2.calcHist([img], [2], None, [256], [0, 256])# 定义Gamma矫正的函数def gamma_trans(img, gamma): # 具体做法是先归一化到1，然后gamma作为指数值求出新的像素值再还原 gamma_table = [np.power(x/255.0, gamma)*255.0 for x in range(256)] gamma_table = np.round(np.array(gamma_table)).astype(np.uint8) # 实现这个映射用的是OpenCV的查找表函数 return cv2.LUT(img, gamma_table)# 执行Gamma矫正，小于1的值让暗部细节大量提升，同时亮部细节少量提升img_corrected = gamma_trans(img, 0.5)cv2.imwrite('img_corrected.jpg',img_corrected)# 分通道计算Gamma矫正后的直方图 256行一列hist_b_corrected = cv2.calcHist([img_corrected], [0], None, [256], [0, 256])hist_g_corrected = cv2.calcHist([img_corrected], [1], None, [256], [0, 256])hist_r_corrected = cv2.calcHist([img_corrected], [2], None, [256], [0, 256])# 这样就会变成一行# -------------------------------# print(hist_b_corrected.reshape(-1))fig = plt.figure(figsize=(12, 6))pix_hists = [ [hist_b.reshape(-1), hist_g.reshape(-1), hist_r.reshape(-1)], [hist_b_corrected.reshape(-1), hist_g_corrected.reshape(-1), hist_r_corrected.reshape(-1)]]pix_vals = range(256)for sub_plt, pix_hist in zip([121, 122], pix_hists): ax = fig.add_subplot(sub_plt, projection='3d') for c, z, channel_hist in zip(['b', 'g', 'r'], [20, 10, 0], pix_hist): cs = [c] * 256 # 传入的 XYZ应该是一维数组, 不能是二维, 应该是一行 ax.bar(pix_vals, channel_hist, zs=z, zdir='y', color=cs, alpha=0.618, edgecolor='none', lw=0) ax.set_xlabel('Pixel Values') ax.set_xlim([0, 256]) ax.set_ylabel('Channels') ax.set_zlabel('Counts')plt.show() output_14_0 接上图 可以看到，Gamma变换后的暗部细节比起原图清楚了很多，并且从直方图来看，像素值也从集中在0附近变得散开了一些。 分离通道由于OpenCV Python和NumPy结合的很紧, 所以即可以使用OpenCV自带的split函数，也可以直接操作numpy数组来分离通道。 123456789101112131415161718# -----------------------------# 分离、合并通道# 即可以使用OpenCV自带的split函数，也可以直接操作numpy数组来分离通道。# # -----------------------------import cv2img = cv2.imread('little_white_dog.jpeg')b, g, r = cv2.split(img)cv2.imshow('Blue', b)cv2.imshow('Green', g)cv2.imshow('Red', r)cv2.waitKey(0)cv2.destroyAllWindows() # split返回RGB三个通道，如果只想返回其中一个通道, 最后的索引指出所需要的通道。b2 = cv2.split(img)[0] 1234567891011121314151617181920# 也可以直接操作NumPy数组来达到这一目的：import cv2import numpy as npimg = cv2.imread('little_white_dog.jpeg')b = np.zeros((img.shape[0], img.shape[1]), dtype=img.dtype)g = np.zeros((img.shape[0], img.shape[1]), dtype=img.dtype)r = np.zeros((img.shape[0], img.shape[1]), dtype=img.dtype)b[:, :] = img[:, :, 0]g[:, :] = img[:, :, 1]r[:, :] = img[:, :, 2]cv2.imshow('Blue', b)cv2.imshow('green', g)cv2.imshow('red', r)cv2.waitKey(0)cv2.destoryAllWindows() 通道合并????????注意：这里只是演示，实际使用时请用OpenCV自带的merge函数！用NumPy组合的结果不能在OpenCV中其他函数使用，因为其组合方式与OpenCV自带的不一样，如下： 12345678910111213141516171819202122import cv2import numpy as npimg = cv2.imread('little_white_dog.jpeg')b = np.zeros((img.shape[0], img.shape[1]), dtype=img.dtype)g = np.zeros((img.shape[0], img.shape[1]), dtype=img.dtype)r = np.zeros((img.shape[0], img.shape[1]), dtype=img.dtype)b[:, :] = img[:, :, 0]g[:, :] = img[:, :, 1]r[:, :] = img[:, :, 2]# openCV 方法 mergemerged = cv2.merge([b, g, r])print('merge by opencv')# 在每个维数上以字节计算的步长print(merged.strides)# numpy 方法 dstackmergedByNp = np.dstack([b, g, r])print('merge by numpy')print(mergedByNp.strides) 1234567891011121314151617# 关于stride# 在每个维数上以字节计算的步长import numpy as np# a数组中每个元素都是NumPy中的整数类型，占8个字节，所以第一维中相邻元素之间的步长为8（个字节）。a = np.arange(6)# (8,)print(a.strides)print(a.dtype) # int64# 从里面开始看，里面是一个4个元素的一维整数数组，所以步长应该为8。外面是一个含有3个元素，每个元素的长度是8×4=32。所以步长为32。b = np.arange(12).reshape(3, 4)b.strides# (160, 40, 8)c = np.arange(60).reshape(3,4,5) # 三维, 行 , 列c.strides Python-OpenCV基础图像的表示单通道的灰度图像在计算机中的表示，就是一个8位无符号整形的矩阵。 在OpenCV中，默认的图像的表示确实反过来的，也就是BGR, 比如在Python中，图像都是用numpy的array表示，但是同样的array在OpenCV中的显示效果和matplotlib中的显示效果就会不一样。 12345678910111213141516import cv2import matplotlib.pyplot as pltimport numpy as npimg = np.array([ [[255, 0, 0], [0, 255, 0], [0, 0, 255]], [[255, 255, 0], [255, 0, 255], [0, 255, 255]], [[255, 255, 255], [128, 128, 128], [0, 0, 0]],], dtype=np.uint8)plt.figure('matplotlib &amp; OpenCV', figsize=(8, 4))# 用matplotlib存储plt.imsave('img_pyplot.jpg', img)# 用OpenCV存储cv2.imwrite('img_cv2.jpg', img) 不管是RGB还是BGR，都是高度×宽度×通道数，H×W×C的表达方式，而在深度学习中，因为要对不同通道应用卷积，所以用的是另一种方式：C×H×W，就是把每个通道都单独表达成一个二维矩阵.np.reshape() 😂 好像就是 CHW (不知道能否这么表述) 像素点太小了, 截图展示 用matplotlib存储: 用OpenCV存储:","categories":[{"name":"OpenCV","slug":"OpenCV","permalink":"http://yaoyirong.cn/categories/OpenCV/"}],"tags":[{"name":"The image processing","slug":"The-image-processing","permalink":"http://yaoyirong.cn/tags/The-image-processing/"}]},{"title":"matplotlib 使用","slug":"matplotlib使用","date":"2017-12-22T03:30:13.000Z","updated":"2017-12-23T08:57:38.000Z","comments":true,"path":"2017/12/22/matplotlib使用/","link":"","permalink":"http://yaoyirong.cn/2017/12/22/matplotlib使用/","excerpt":"python-matplotlib","text":"python-matplotlib python matplotlib jupytermatplotlib 是 python 的 著名数据可视化工具包 2D图表Matplotlib中最基础的模块是pyplot。 1%matplotlib inline 1234567891011121314151617181920212223242526272829303132333435363738394041# 有一组数据，还有一个拟合模型，通过下面的代码图来可视化import numpy as npimport matplotlib as mplimport matplotlib.pyplot as plt# 通过rcParams设置全局横纵轴字体大小mpl.rcParams['xtick.labelsize'] = 24mpl.rcParams['ytick.labelsize'] = 24np.random.seed(42)# x 轴的采样点x = np.linspace(0, 5, 100)# 通过下面曲线加上噪声生成数据，所以拟合模型就用y了……y = 2*np.sin(x) + 0.3*x**2y_data = y + np.random.normal(scale=0.3, size=100)# figure() 指定图表名称plt.figure('data')# '.'标明画散点图，每个散点的形状是个圆plt.plot(x, y_data, '.')# 画模型的图，plot函数默认画连线图plt.figure('model')plt.plot(x, y)# 两个图画一起plt.figure('data &amp; model')# 通过'k'指定线的颜色，lw指定线的宽度# 第三个参数除了颜色也可以指定线形，比如'r--'表示红色虚线# 更多属性可以参考官网：http://matplotlib.org/api/pyplot_api.htmlplt.plot(x, y, 'k', lw=3)# scatter可以更容易地生成散点图plt.scatter(x, y_data)# 将当前figure的图保存到文件result.pngplt.savefig('result.png')plt.show() output_3_0 output_3_0 output_3_2 点和线图表只是最基本的用法，有的时候我们获取了分组数据要做对比，柱状或饼状类型的图 平时画图蹦出的一个窗口，这叫一个figure。Figure相当于一个大的画布，在每个figure中，又可以存在多个子图，这种子图叫做axes。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import matplotlib as mplimport numpy as npimport matplotlib.pyplot as plt# 全都是设置字的大小mpl.rcParams['axes.titlesize'] = 13mpl.rcParams['xtick.labelsize'] = 10mpl.rcParams['ytick.labelsize'] = 10mpl.rcParams['axes.labelsize'] = 10mpl.rcParams['xtick.major.size'] = 0mpl.rcParams['ytick.major.size'] = 0# 包含了狗，猫和猎豹的最高奔跑速度，还有对应的可视化颜色speed_map = &#123; 'dog': (48, '#7199cf'), 'cat': (45, '#4fc4aa'), 'cheetah': (120, '#e1a7a2')&#125;# 整体图的标题fig = plt.figure('Bar chart &amp; Pie chart', figsize=(12,6))# 在整张图上加入一个子图，121的意思是在一个1行2列的子图中的第一张ax = fig.add_subplot(121)ax.set_title('Running speed - bar chart')# 生成x轴每个元素的位置xticks = np.arange(3)# 定义柱状图每个柱的宽度bar_width = 0.5# 动物名称animals = speed_map.keys()# 奔跑速度speeds = [x[0] for x in speed_map.values()]# 对应颜色colors = [x[1] for x in speed_map.values()]# 画柱状图，横轴是动物标签的位置，纵轴是速度，定义柱的宽度，同时设置柱的边缘为透明bars = ax.bar(xticks, speeds, width=bar_width, edgecolor='none')# 设置y轴的标题ax.set_ylabel('Speed(km/h)')# x轴每个标签的具体位置，设置为每个柱的中央ax.set_xticks(xticks)# 设置每个标签的名字ax.set_xticklabels(animals)# 设置x轴的范围 ---轴不变, 在轴上显示多少, 直接决定柱子好不好看ax.set_xlim([bar_width/2-1, 3-bar_width/2])# 设置y轴的范围ax.set_ylim([0, 125])# 给每个bar分配指定的颜色for bar, color in zip(bars, colors): bar.set_color(color) # 在122位置加入新的图ax = fig.add_subplot(122)ax.set_title('Running speed - pie chart')# 生成同时包含名称和速度的标签labels = ['&#123;&#125;\\n&#123;&#125; km/h'.format(animal, speed) for animal, speed in zip(animals, speeds)]# 画饼状图，并指定标签和对应颜色ax.pie(speeds, labels=labels, colors=colors)plt.show() output_5_0 3D 图表Matplotlib中也能支持一些基础的3D图表，比如曲面图，散点图和柱状图。这些3D图表需要使用mpl_toolkits模块 python2 python3 原来1/2（两个整数相除）结果是0，现在是0.5了python 2.2+ 以上都可以使用 from future import division 实现改特性, 同时注意 // 取代了之前的 / 运算 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import numpy as npimport matplotlib.pyplot as plt# 3D图标必须的模块，project='3d'的定义from mpl_toolkits.mplot3d import Axes3Dnp.random.seed(42)n_grids = 51 # x-y平面的格点数 c = n_grids//2 # 中心位置nf = 2 # 低频成分的个数# 生成格点x = np.linspace(0, 1, n_grids)y = np.linspace(0, 1, n_grids)# x和y是长度为n_grids的array# meshgrid会把x和y组合成n_grids*n_grids的array，X和Y对应位置就是所有格点的坐标X, Y = np.meshgrid(x, y)# 生成一个0值的傅里叶谱spectrum = np.zeros((n_grids, n_grids), dtype=np.complex)# 生成一段噪音，长度是(2*nf+1)**2/2#noise = [np.complex(x, y) for x, y in np.random.uniform(-1, 1, ((2*nf+1)**2/2, 2))]noise = [np.complex(x, y) for x, y in np.random.uniform(-1,1,((2*nf+1)**2//2, 2))]# 傅里叶频谱的每一项和其共轭关于中心对称noise_block = np.concatenate((noise, [0j], np.conjugate(noise[::-1])))# 将生成的频谱作为低频成分spectrum[c-nf:(c+nf+1), c-nf:c+nf+1] = noise_block.reshape((2*nf+1, 2*nf+1))# 进行反傅里叶变换Z = np.real(np.fft.ifft2(np.fft.ifftshift(spectrum)))# 创建图表fig = plt.figure('3D surface &amp; wire', figsize=(12, 6))# 第一个子图，surface图ax = fig.add_subplot(121, projection='3d')# alpha定义透明度，cmap是color map# rstride和cstride是两个方向上的采样，越小越精细，lw是线宽ax.plot_surface(X, Y, Z, alpha=0.7, cmap='jet', rstride=1, cstride=1, lw=0)# 第二个子图，网线图ax = fig.add_subplot(122, projection='3d')ax.plot_wireframe(X, Y, Z, rstride=3, cstride=3, lw=0.5)plt.show() output_7_0 3D的散点图 也是常常用来查看空间样本分布的一种手段，并且画起来比表面图和网线图更加简单 这个例子中，为了方便，直接先采样了一堆3维的正态分布样本，保证方向上的均匀性。然后归一化，让每个样本到原点的距离为1，相当于得到了一个均匀分布在球面上的样本。再接着把每个样本都乘上一个均匀分布随机数的开3次方，这样就得到了在球体内均匀分布的样本，最后根据判别平面3x+2y-z-1=0对平面两侧样本用不同的形状和颜色画出，图像如下： shape[0] 看行数, 即第一维数 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import matplotlib.pyplot as pltimport numpy as npfrom mpl_toolkits.mplot3d import Axes3Dnp.random.seed(42)# 采样个数500n_samples = 500dim = 3# 先生成一组3维正态分布数据，数据方向完全随机samples = np.random.multivariate_normal( np.zeros(dim), np.eye(dim), n_samples)# 通过把每个样本到原点距离和均匀分布吻合得到球体内均匀分布的样本# samples[i] / np.linalg.norm(samples[i]) 归一化 (二范数是向量长度)# r 一个均匀分布随机数的开3次方for i in range(samples.shape[0]): r = np.power(np.random.random(), 1.0/3.0) samples[i] *= r / np.linalg.norm(samples[i])upper_samples = []lower_samples = []for x, y, z in samples: # 3x+2y-z=1作为判别平面 if z &gt; 3*x + 2*y - 1: upper_samples.append((x, y, z)) else: lower_samples.append((x, y, z)) fig = plt.figure('3D scatter plot')ax = fig.add_subplot(111, projection='3d')uppers = np.array(upper_samples)lowers = np.array(lower_samples)# 用不同颜色不同形状的图标表示平面上下的样本# 判别平面上半部分为红色圆点，下半部分为绿色三角ax.scatter(uppers[:, 0], uppers[:, 1], uppers[:, 2], c='r', marker='o')ax.scatter(lowers[:, 0], lowers[:, 1], lowers[:, 2], c='g', marker='^')plt.show() output_9_0 1234x = np.linspace(0, 1, 10)y = np.linspace(2, 3, 10)X, Y = np.meshgrid(x, y)X.shape[1] 110 123456789101112131415161718# 展示画一个3d图import matplotlib.pyplot as plt# 导入3D 包from mpl_toolkits.mplot3d import Axes3D# 将会话框进行对象化fig = plt.figure()# 将对话框划分为一个子图, 并指定为3d图ax = fig.add_subplot(111, projection='3d')# 定义 x, y, z 三个坐标轴的数据集U = [1, 1, 2, 2]V = [3, 4, 4, 3]W = [1, 100, 1, 1]# 用函数填满 4个点组成的三角形空间ax.plot_trisurf(U, V, W)plt.show() output_11_0 图像显示Matplotlib也支持图像的存取和显示，并且和OpenCV一类的接口比起来，对于一般的二维矩阵的可视化要方便很多： 这段代码中第一个例子是读取一个本地图片并显示，第二个例子中直接把上小节中反傅里叶变换生成的矩阵作为图像拿过来，原图和经过乘以3再加4变换的图直接绘制了两个形状一样，但是值的范围不一样的图案。显示的时候imshow会自动进行归一化，把最亮的值显示为纯白，最暗的值显示为纯黑。这是一种非常方便的设定，尤其是查看深度学习中某个卷积层的响应图时。得到图像如下： 1234567891011121314151617181920import matplotlib.pyplot as plt# 读取一张手机的照片并显示plt.figure('A little White iPhone')little_iPhone_img = plt.imread('iPhone.png')plt.imshow(little_iPhone_img)# Z是上小节生成的随机图案，img0就是Z，img1是Z做了个简单的变换img0 = Zimg1 = 3*Z + 4# cmap指定为'gray'用来显示灰度图fig = plt.figure('Auto Normalized Visualization')ax0 = fig.add_subplot(121)ax0.imshow(img0, cmap='gray')ax1 = fig.add_subplot(122)ax1.imshow(img1, cmap='gray')plt.show() output_13_0 output_13_1","categories":[{"name":"matplotlib","slug":"matplotlib","permalink":"http://yaoyirong.cn/categories/matplotlib/"}],"tags":[{"name":"Data mining python","slug":"Data-mining-python","permalink":"http://yaoyirong.cn/tags/Data-mining-python/"}]},{"title":"numpy 使用","slug":"numpy使用","date":"2017-12-22T02:30:13.000Z","updated":"2017-12-23T07:10:17.000Z","comments":true,"path":"2017/12/22/numpy使用/","link":"","permalink":"http://yaoyirong.cn/2017/12/22/numpy使用/","excerpt":"python-numpy 基础","text":"python-numpy 基础 入门 numpy 篇numpy 篇array，也就是数组，是numpy中最基础的数据结构, 最关键的属性是维度和元素类型, 在numpy中，可以非常方便地创建各种不同类型的多维数组，并且执行一些基本基本操作 array 维度 元素类型 基本类型（array)1%matplotlib inline 12#%load douban.py# run douban.py 1!python --version 1Python 2.7.10 创建数组, 获取数组的属性12345678910111213141516171819202122232425262728293031323334import matplotlib.pyplot as pltimport numpy as npnp.arange(15).reshape(3,5)# 在从1到3中产生9个数：(等差数列)np.linspace(1, 3, 9)# 创建矩阵np.zeros((3, 4))# 2x2x3的无符号8位整型3维数组，并且初始化所有元素值为0g = np.zeros((2, 2, 3), dtype=np.uint8)print(g) # 用另一种类型表示g.astype(np.float)np.ones((3, 4))np.eye(3)# 创建一个一维数组，元素值是把3重复4次，array([3, 3, 3, 3])np.repeat(3, 4)# 三维数组a = np.zeros((2,2,2))# 数组的维数a.ndim# 数组每一维的大小a.shape# 数组的元素数a.size# 元素类型 dtype('float64')a.dtype# 每个元素所占的字节数 8a.itemsize 12345[[[0 0 0] [0 0 0]] [[0 0 0] [0 0 0]]] 18 load()和save()用Numpy专用的二进制格式保存数据，它们会自动处理元素类型和形状等信息。savez()提供了将多个数组存储至一个文件的能力，调用load()方法返回的对象，可以使用数组名对各个数组进行读取。默认数组名arr_0,arr_1,arr_2…… 123456789101112# 关于文件# 保存到文件# 保存为二进制 第二个参数为要存的数组np.save('p.npy', a)# 保存为txtnp.savetxt('001', (11, 12, 13))# 读取数据q = np.load('p.npy')qq = np.loadtxt('001')print(q)print(qq) 1[ 11. 12. 13.] 数组索引，切片，赋值12345678910111213141516171819202122232425# 切片不能直接printa = np.array([[2,3,4], [5,6,7]])print(a)# 索引, 行列b = a[1, 2]print(b)# 切片, 得到数组b = a[1,:]print(b)# 切片, 得到数组b = a[1, 1:2]print(b)a[1,:] = [8, 9, 10]print(a)# 平均分成3份g = np.split(np.arange(9), 3)print(g)# 按照下标位置进行划分h = np.split(np.arange(9), [2, -3])# 使用 for 操作元素for x in np.linspace(1, 3, 3): print(x) 1234567891011[[2 3 4] [5 6 7]]7[5 6 7][6][[ 2 3 4] [ 8 9 10]][array([0, 1, 2]), array([3, 4, 5]), array([6, 7, 8])]1.02.03.0 基本的数组运算123456789a = np.ones((2, 2))b = np.eye(2)print(a&gt;2)print(a+b)print(a-b)print(b*2)print((a*2)*(b*2))print(b/(a*2))print((a*2)**4) 1234567891011121314[[False False] [False False]][[ 2. 1.] [ 1. 2.]][[ 0. 1.] [ 1. 0.]][[ 2. 0.] [ 0. 2.]][[ 4. 0.] [ 0. 4.]][[ 0.5 0. ] [ 0. 0.5]][[ 16. 16.] [ 16. 16.]] numerical python，基础数学运算也是强大的: 1234567891011np.abs(-1) np.sin(np.pi/2) np.arctanh(0.462118)d = np.exp(3)f = np.power(2, 3)g = np.dot([1, 2], [3, 4])h = np.sqrt(25)l = np.sum([1, 2, 3, 4])m = np.mean([4, 5, 6, 7])# 标准差p = np.std([1, 2, 3, 2, 1, 3, 2, 0]) 12345678# 使用数组对象自带的方法print(a.sum())# 计算每一列的和（二维数组中类似于矩阵的列）的和print(a.sum(axis=0))a.min()a.max()a.mean() 123.0[ 2. 1.] 10.75 12345678910111213141516171819202122232425262728293031# 使用numpy下的方法：print(np.sin(a))print(np.max(a))# 向下取整, 取不大于x的整数print(np.floor(a))print(np.exp(a))# 矩阵乘法print(np.dot(a, a))print('-----')print(a)print(b)# 合并数组# 使用numpy下的vstack和hstack函数：# 只能同行同列'''vstack是指沿着纵轴拼接两个array，verticalhstack是指沿着横轴拼接两个array，horizontal更广义的拼接用concatenate实现，horizontal后的两句依次等效于vstack和hstackstack不是拼接而是在输入array的基础上增加一个新的维度'''r = np.concatenate((a, b), axis=-1)c = np.vstack((a, b))print(c)d = np.hstack((a, b))print(d)# 深拷贝# 即更改 a b 数, c不会发生改变 1234567891011121314151617181920[[ 0.84147098 0.84147098] [ 0.84147098 0. ]]1.0[[ 1. 1.] [ 1. 0.]][[ 2.71828183 2.71828183] [ 2.71828183 1. ]][[ 2. 1.] [ 1. 1.]]-----[[ 1. 1.] [ 1. 0.]][[ 1. 1.] [ 1. 0.]][[ 1. 1.] [ 1. 0.] [ 1. 1.] [ 1. 0.]][[ 1. 1. 1. 1.] [ 1. 0. 1. 0.]] 123456789101112# 数组对象自带了浅拷贝和深拷贝的方法，但是一般用深拷贝多一些# 浅拷贝a = np.ones((2, 2))b = aa[1,1] = 4print(b is a)print(b)# 深拷贝c = a.copy()a[1, 1] = 0print(c) 12345True[[ 1. 1.] [ 1. 4.]][[ 1. 1.] [ 1. 4.]] 123456789101112131415161718# 转置e = np.array([[2, 3], [5, 6]])print(e)print(e.transpose())# 按指定轴进行转置t = e.transpose((2, 0, 1))# 逆时针旋转90度，第二个参数是旋转次数v = np.rot90(e, 3)# 沿纵轴左右翻转w = np.fliplr(e)# 沿水平轴上下翻转x = np.flipud(e)# 按照一维顺序滚动位移y = np.roll(e, 1)# 按照指定轴滚动位移z = np.roll(e, 1, axis=1)# 迹np.trace(a) 1234[[2 3] [5 6]][[2 5] [3 6]] 11.0 1234567# numpy.linalg模块中有很多关于矩阵运算的方法：import numpy.linalg as nplg# 特征值 特征向量nplg.eig(a)print(a) 12[[ 1. 1.] [ 1. 0.]] 12345678910c = np.array([[1,2], [3,4]])# 每一列的最大值c.max(axis=1)# 每一行的均值c.mean(axis=0) # 展开一个numpy数组为1维数组，array([1, 2, 3, 4])c.flatten()# 展开一个可以解析的结构为1维数组，array([1, 2, 3, 4]) np.ravel(c) 1array([1, 2, 3, 4]) 线性代数模块 (linalg)在深度学习相关的数据处理和运算中，线性代数模块（linalg）是最常用的之一. 结合numpy提供的基本函数，可以对向量，矩阵，或是说多维张量进行一些基本的运算. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import numpy as npa = np.array([3, 4])# norm则表示范数，首先需要注意的是范数是对向量（或者矩阵）的度量，是一个标量（scalar）：np.linalg.norm(a)b = np.array([ [1, 2, 3], [4, 5, 6], [7, 8, 9]])c = np.array([1, 0, 1])# 矩阵和向量之间的乘法 ???np.dot(b, c)np.dot(c, b.T)# 求矩阵的迹，15(一个n×n矩阵A的主对角线（从左上方至右下方的对角线）# 上各个元素的总和被称为矩阵A的迹)np.trace(b)# 求矩阵的行列式值，0np.linalg.det(b)# 求矩阵的秩，2，不满秩，因为行与行之间等差np.linalg.matrix_rank(b)d = np.array([ [2, 1], [1, 2]])'''对正定矩阵求本征值和本征向量本征值为u，array([ 3., 1.])本征向量构成的二维array为v，array([[ 0.70710678, -0.70710678], [ 0.70710678, 0.70710678]])是沿着45°方向eig()是一般情况的本征值分解，对于更常见的对称实数矩阵，eigh()更快且更稳定，不过输出的值的顺序和eig()是相反的'''u, v = np.linalg.eig(d)print(u, v)# Cholesky分解并重建# 把一个对称正定的矩阵表示成一个下三角矩阵L和其转置的乘积的分解l = np.linalg.cholesky(d)np.dot(l, l.T)e = np.array([ [1, 2], [3, 4]])# 对不镇定矩阵，进行SVD分解(奇异值分解)并重建U, s, V = np.linalg.svd(e)S = np.array([ [s[0], 0], [0, s[1]]])np.dot(U, np.dot(S, V)) 12[ 3. 1.] [[ 0.70710678 -0.70710678] [ 0.70710678 0.70710678]] 12array([[ 1., 2.], [ 3., 4.]]) 随机模块（random）包含了随机数产生和统计分布相关的基本函数, Python本身也有随机模块random，不过功能更丰富 注意: 方法参数中都是规定产生随机数的数量(一维, 二维a*b), 并非是产生的大小 随机数产生123456789101112131415161718192021222324252627282930import numpy as npimport numpy.random as random# 设置随机数种子random.seed(42)# 产生一个1x3 数组，[0,1)之间的浮点型随机数random.rand(1, 3)# 产生一个[0,1)之间的浮点型随机数random.random()# 下边4个没有区别，都是按照指定大小产生[0,1)之间的浮点型随机数array，不Pythonic…random.random((3, 3))random.sample((3, 3))random.random_sample((3, 3))random.ranf((3, 3))# 产生10个[1,6)之间的浮点型随机数5*random.random(10) + 1# (3, 3)生成二维random.uniform(1, 6, 10) # 产生10个[1,6)之间的整型随机数random.randint(1, 6, 10)# 产生2x5的标准正态分布样本random.normal(size=(5, 2))# 产生5个，n=5，p=0.5的二项分布样本random.binomial(n=5, p=0.5, size=5) 1array([3, 4, 4, 2, 1]) 统计分布对具体的样本数据进行与统计分布相关操作 in-place操作，意思是所有的操作都是”就地“操作，不允许进行移动，或者称作 原位操作，即不允许使用临时变量。 1234567891011121314151617a = np.arange(10)# 从a中有回放的随机采样7个random.choice(a, 7)# 从a中无回放的随机采样7个random.choice(a, 7, replace=False)# 对a进行乱序并返回一个新的arrayrandom.permutation(a)# 对a进行in-place乱序random.shuffle(a)print(a)# 生成一个长度为9的随机bytes序列并作为str返回random.bytes(9) 1[1 8 4 2 9 6 5 7 0 3] 1b&apos;[4&amp;`a&#125;0\\x94A&apos;","categories":[{"name":"numpy","slug":"numpy","permalink":"http://yaoyirong.cn/categories/numpy/"}],"tags":[{"name":"Data mining","slug":"Data-mining","permalink":"http://yaoyirong.cn/tags/Data-mining/"}]},{"title":"LeetCode1541","slug":"LeetCode算法刷题过程","date":"2017-03-15T07:03:02.000Z","updated":"2017-12-22T12:54:14.000Z","comments":true,"path":"2017/03/15/LeetCode算法刷题过程/","link":"","permalink":"http://yaoyirong.cn/2017/03/15/LeetCode算法刷题过程/","excerpt":"","text":"LeetCode(一)Tag: Arrays 和Hash (1,15)1. twoSum 返回目标值索引 本题学习过程帮助我了解vector, unordered_map和map 我的思路: 先对vectornum进行sort排序, 进而头尾相加, 小于target则头指针加一; 大于则只尾指针减一; 等于跳出循环 1234567891011121314151617181920212223242526#include &lt;iostream&gt;#include &lt;algorithm&gt;#include &lt;vector&gt;using namespace std;vector&lt;int&gt; twoSum(vector&lt;int&gt;&amp; nums, int target) &#123; vector&lt;int&gt;vec(nums); vector&lt;int&gt; result ; sort(nums.begin(), nums.end()); int a,b,c=0; for(int i=0,j=nums.size()-1; i&lt;=j; i++,j--) &#123; int temp = nums.at(i) + nums.at(j); if(temp &lt; target) &#123;j++;&#125; else if(temp &gt; target)&#123;i--;&#125; else &#123;a=nums.at(i); b=nums.at(j);break;&#125; &#125; for(int k=0;k&lt;vec.size();k++)&#123; if(c==2) break; if(vec.at(k) == a) &#123;result.push_back(k);c++; continue;&#125; if(vec.at(k) == b) &#123;result.push_back(k);c++; continue;&#125; &#125; return result;&#125; 网上解答 Solution/*将n方复杂度的转换为linear, 主要是使用map追踪正确需要的数, 把需要的值存进map 12345678910111213vector&lt;int&gt; twoSum(vector&lt;int&gt; &amp;nums, int target)&#123; unordered_map&lt;int, int&gt; hash; vector&lt;int&gt; result; for(int i=0; i&lt;nums.size(); i++)&#123; int numToFind = target - nums[i]; if(hash.find(numsToFind) != hash.end())&#123; result.push_back(hash[numsToFind]); result.push_back(i); return result; &#125; hash[nums[i]] = i; &#125;&#125; ####3. 15. 3Sum 1234报错Runtime Error Message: reference binding to null pointer of type &apos;struct value_type&apos;原因: 没有验证输入是否为空数组(向量)解决:方法里加入if判断 我的思路: 参照第一题, 先外层遍历i, target=0-nums[i], 在按照第一题的方法去做 问题: 内层时需要考虑不中间返回, 以及去重问题, 耽误了我的时间复杂度, 😭😭😭 开始 但是,超时了妈个叽 12345678910111213141516171819202122232425262728293031323334353637class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; threeSum(vector&lt;int&gt;&amp; nums) &#123; set&lt;int&gt; hash; //便于追踪正确值, 两个的情况 set&lt;int&gt; skip; //存放已测试的num[i],用于跳过 vector&lt;vector&lt;int&gt;&gt; result; vector&lt;int&gt; tmp; set&lt;int&gt; repeat; if (nums.size() &lt; 3)&#123; return result; &#125; for (int i = 0; i &lt; nums.size()-2; i++) &#123; if (skip.find(nums[i]) != skip.end()) continue; int target = 0 - nums[i]; hash.clear(); for (int j = i + 1; j &lt; nums.size(); j++) &#123; int numToFind = target - nums[j]; if (skip.find(nums[j]) != skip.end() || skip.find(numToFind) != skip.end()) continue; if(hash.find(nums[j]) != hash.end())&#123; if(numToFind != nums[j]) continue; if(repeat.find(numToFind) != repeat.end()) continue; repeat.insert(numToFind); &#125; if (hash.find(numToFind) != hash.end()) &#123; tmp.push_back(numToFind); tmp.push_back(nums[j]); tmp.push_back(nums[i]); result.push_back(tmp); tmp.clear(); &#125; hash.insert(nums[j]); &#125; skip.insert(nums[i]); &#125; return result; &#125;&#125;; 123456789101112131415161718192021222324252627282930313233//改成能accepted, 然而复杂度不忍直视😂 vector&lt;vector&lt;int&gt;&gt; threeSum(vector&lt;int&gt;&amp; nums) &#123; set&lt;int&gt; hash; //便于追踪正确值, 两个的情况 vector&lt;vector&lt;int&gt;&gt; result; vector&lt;int&gt; tmp; if (nums.size() &lt; 3) &#123; return result; &#125; sort(nums.begin(), nums.end()); for (int i = 0; i &lt; nums.size() - 2; i++) &#123; if ((i &gt; 0 &amp;&amp; nums[i] != nums[i - 1]) || i == 0) &#123; int target = 0 - nums[i]; hash.clear(); int j = i + 1; while(j &lt; nums.size()) &#123; int numToFind = target - nums[j]; if(hash.find(numToFind) != hash.end())&#123; tmp.push_back(numToFind); tmp.push_back(nums[j]); tmp.push_back(nums[i]); result.push_back(tmp); tmp.clear(); while(j &lt; nums.size()-1 &amp;&amp; nums[j] == nums[j+1]) j++; j++; &#125; else hash.insert(nums[j++]); &#125; // for(it = hash.begin(); it!=hash.end(); it++)&#123;cout&lt;&lt;*it&lt;&lt;\" \";&#125; &#125; // cout&lt;&lt;endl; &#125; return result; &#125; 参照网上的答案更改过后: 问题明白两点 排序,先对给定数组进行排序 有时, while循环比for循环更好用 优势在于更好的控制每一次 i+1 1234567891011121314151617181920212223242526vector&lt;vector&lt;int&gt;&gt; threeSum2(vector&lt;int&gt; &amp;nums) &#123; sort(nums.begin(), nums.end()); vector&lt;int&gt; tmp; vector&lt;vector&lt;int&gt;&gt; res; for(int i = 0; i &lt; nums.size(); i++)&#123; if(i == 0 || (i &gt; 0 &amp;&amp; nums[i] != nums[i-1]))&#123; int lo = i + 1, hi= nums.size() - 1, sum = 0 - nums[i]; while (lo &lt; hi)&#123; //第一次允许, 以后通过排序直接跳过可能的重复 if (nums[lo] + nums[hi] == sum)&#123; tmp.push_back(nums[i]); tmp.push_back(nums[lo]); tmp.push_back(nums[hi]); res.push_back(tmp); tmp.clear(); while (lo &lt; hi &amp;&amp; nums[lo] == nums[lo + 1]) lo++; while (lo &lt; hi &amp;&amp; nums[hi] == nums[hi - 1]) hi--; lo++; hi--; &#125; else if(nums[lo] + nums[hi] &lt; sum) lo++; else hi--; &#125; &#125; &#125; return res;&#125; char p = “test”; 这个声明，声明了一个指针，而这个指针指向的是*全局的const内存区，const内存区当然不会让你想改就改的。 541.Reverse String II问题描述：一串小写字符，分","categories":[{"name":"LeetCode","slug":"LeetCode","permalink":"http://yaoyirong.cn/categories/LeetCode/"}],"tags":[{"name":"LeetCode","slug":"LeetCode","permalink":"http://yaoyirong.cn/tags/LeetCode/"},{"name":"arrays","slug":"arrays","permalink":"http://yaoyirong.cn/tags/arrays/"},{"name":"hash","slug":"hash","permalink":"http://yaoyirong.cn/tags/hash/"}]},{"title":"范数","slug":"机器学习第一周之范数","date":"2017-03-01T02:20:13.000Z","updated":"2017-04-17T13:05:03.000Z","comments":true,"path":"2017/03/01/机器学习第一周之范数/","link":"","permalink":"http://yaoyirong.cn/2017/03/01/机器学习第一周之范数/","excerpt":"机器学习?(ノへ￣、)","text":"机器学习?(ノへ￣、) 向量范数min∑(f(x)-y)^2+正则表达(范数解决过拟合问题) L0 特征选择, 非0元素稀疏, 缺点是难优化 L1 可以直接使得元素值为0从而容易稀疏 L2 岭回归 最小化L2范数,可以使得x的元素值都很小, 但都不是0 最小二乘L2范数优化, 凸优化方法 优点: 1) 改善”过拟合”overfitting—新样本表现很差(长发问题) 2) 利于优化 机器学习中有时候损失函数是非凸的, 例如: 神经网络, 梯度下降之类的方法遇见 矩阵范数 1-范数 列范数最大 变成向量 2-范数 谱范数 难优化 求特征值 F-范数: (∑i=1,m∑j1,n aij^2)^1/2 最小化矩阵的f范数,会使得矩阵的每个元素都很小,近0 ||A-B||F A,B矩阵尽可能相同 应用字典学习 核范数—-矩阵奇异值的和 最小化可以导致矩阵低秩 矩阵的秩—-矩阵线性不相关的行数 去除冗余 不好求所以 近似 是核范数 应用 推荐系统,低秩 奇异值分解 /鲁棒PCA 重构为低秩+噪声, 不在原始图像上降维 2,1范数 按列求2范数(平方开根)再求1范数, 整列(列向量)稀疏(全0) Lasso 找出关键词 Group Lasso使一组为0,找出关键句子 Hierarchical Lasso找出关键段 应用:文本分类 1,2范数 按列1(每一列可能有好多0)再2(不稀疏保证了每一列不为0,进而保证了每一列中的每一行不可全为0,平方和开根不为0), 使得行内元素互斥, 行内有0元素但不可能全0","categories":[{"name":"norm","slug":"norm","permalink":"http://yaoyirong.cn/categories/norm/"}],"tags":[{"name":"Machine learning","slug":"Machine-learning","permalink":"http://yaoyirong.cn/tags/Machine-learning/"}]},{"title":"Mac Linux文件系统","slug":"mac文件系统","date":"2016-07-15T01:18:35.000Z","updated":"2017-04-17T13:30:29.000Z","comments":true,"path":"2016/07/15/mac文件系统/","link":"","permalink":"http://yaoyirong.cn/2016/07/15/mac文件系统/","excerpt":"","text":"mac 文件系统隐藏cmd中ls / 符合unix传统的目录/bin 传统unix命令的存放目录，如ls，rm，mv等。/sbin 传统unix管理类命令存放目录，如fdisk，ifconfig等等。/usr 第三方程序安装目录。/usr/bin, /usr/sbin, /usr/lib，其中/usr/lib目录中存放了共享库（动态链接库）./etc. 标准unix系统配置文件存放目录，如用户密码文件/etc/passwd。此目录实际为指向/private/etc的链接。/dev 设备文件存放目录，如何代表硬盘的/dev/disk0。/tmp 临时文件存放目录，其权限为所有人任意读写。此目录实际为指向/private/tmp的链接。/var 存放经常变化的文件，如日志文件。此目录实际为指向/private/var的链接。 os x特有的目录/Applications 应用程序目录，默认所有的GUI应用程序都安装在这里；/Library 系统的数据文件、帮助文件、文档等等；/Network 网络节点存放目录；/System 他只包含一个名为Library的目录，这个子目录中存放了系统的绝大部分组件，如各种framework，以及内核模块，字体文件等等。/Users 存放用户的个人资料和配置。每个用户有自己的单独目录。/Volumes 文件系统挂载点存放目录。/cores 内核转储文件存放目录。当一个进程崩溃时，如果系统允许则会产生转储文件。/private 里面的子目录存放了/tmp, /var, /etc等链接目录的目标目录。 OS X发生崩溃和不能启动的概率实在是太低了，就算是系统出现问题，由于用户目录和系统目录是彼此独立的，所以也容易找回。所以通常情况下，用户直接把资料存放在自己的用户目录中 Linux 文件系统形式表现上一体——所有数据目录均为根目录下的子目录实质——多个不同的【逻辑主体】（为了实现不同的逻辑功能）组合在一起 文件系统结构标准LINUX系统的数据文件分类——双重标准共享的与独享的（shareable vs. unshareable）数据与平台不相关，如/usr （共享） 数据是平台相关的，如配置数据/etc。变化的和静态的（variable vs. static）相对非管理员用户而言的，没有更变的权限 （静态） Linux根文件系统逻辑组成（以具体目录表现逻辑功能）/ ——根目录（专用的静态的“根本所在”）/usr ——（共享的静态的）/var——（动态的共享的）/opt/home 根目录必选组成目录 /bin 基本工具或命令/sbin 系统管理工具或命令/etc 主机相关（Host-speciﬁc）的配置数据/lib 基本共享库和内核模块/dev 设备文件/boot 引导程序/root 管理员的工作目录/mnt 系统管理员的临时挂接点/tmp 系统级临时文件/media 移动设备挂接点 /usr 目录/usr 目录是系统的一大组成部分，共享的静态的 静态：普通用户不可更改/usr下数据共享：/usr下数据与主机平台不相关，代码独立于运行主机/usr必选组成目录/usr/bin Most user commands/usr/include Header ﬁles included by C programs/usr/blib Libraries/usr/local Local hierarchy (empty after main installation)/usr/sbin Non-vital system binaries/usr/share Architecture-independent data /var目录/var基本上是动态的和共享的（少量是主机相关的，如/var/log）。/var保存大部分是程序运行期的动态生成数据","categories":[{"name":"杂","slug":"杂","permalink":"http://yaoyirong.cn/categories/杂/"}],"tags":[{"name":"os","slug":"os","permalink":"http://yaoyirong.cn/tags/os/"}]}]}